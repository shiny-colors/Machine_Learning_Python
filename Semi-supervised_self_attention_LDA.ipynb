{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import scipy.linalg\n",
    "import itertools\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from scipy.stats import norm\n",
    "from numpy.random import *\n",
    "from scipy import optimize\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項分布の乱数を生成する関数\n",
    "def rmnom(pr, n, k, pattern):\n",
    "    if pattern==1:\n",
    "        z_id = np.array(np.argmax(np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis], axis=1), dtype=\"int\")\n",
    "        Z = np.diag(np.repeat(1, k))[z_id, ]\n",
    "        return z_id, Z\n",
    "    z_id = np.array(np.argmax((np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis]), axis=1), dtype=\"int\")\n",
    "    return z_id\n",
    "\n",
    "# ディリクリ分布の乱数を生成する関数\n",
    "def Dirichlet(alpha, n):\n",
    "    x = torch.Tensor(np.random.dirichlet(alpha, n))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの設定\n",
    "# パラメータ数を定義\n",
    "types = 2\n",
    "min_word = 5\n",
    "max_word = 50\n",
    "k11 = 5   # topic wordのsyntax数\n",
    "k12 = 7   # general wordのsyntax数\n",
    "k1 = k11 + k12   # syntax数\n",
    "k21 = 15   # topic wordのトピック数\n",
    "k22 = 10   # general wordのトピック数\n",
    "k2 = k21 + k22   # topic数\n",
    "k3 = 15   # word classのトピック数\n",
    "d = 5000   # 文書数\n",
    "v11 = 1000  # topic wordのvocabulary数\n",
    "v12 = 400   # general wordのvocabulary数\n",
    "v1 = v11 + v12   # vocabulary数\n",
    "v2 = 100   # word class数\n",
    "\n",
    "# 文書データの統計量を生成\n",
    "pt = np.random.poisson(np.random.gamma(12.5, 1.0, d), d)\n",
    "pt[pt < 5] = 5\n",
    "M = np.sum(pt)\n",
    "w = np.random.poisson(np.random.gamma(17.5, 1.5, np.sum(pt)), np.sum(pt))\n",
    "w[w < min_word] = min_word\n",
    "N = np.sum(w)\n",
    "\n",
    "# 行列演算ベクトルを定義\n",
    "k_vec1 = np.repeat(1.0, k1)\n",
    "k_vec2 = np.repeat(1.0, k2)\n",
    "k_vec21 = np.repeat(1.0, k21)\n",
    "k_vec22 = np.repeat(1.0, k22)\n",
    "index_k11 = np.arange(k11)\n",
    "index_k12 = np.arange(k12) + k11\n",
    "index_k21 = np.arange(k21)\n",
    "index_k22 = np.arange(k22) + k21\n",
    "index_v11 = np.arange(v11)\n",
    "index_v12 = np.arange(v12) + v11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDとインデックスを定義\n",
    "# IDの定義\n",
    "m = np.repeat(0, d)\n",
    "doc_list = []\n",
    "d_id = []\n",
    "doc_id = np.repeat(np.arange(d), pt)\n",
    "for i in range(d):\n",
    "    doc_list.append(np.where(doc_id==i)[0].astype(\"int\"))\n",
    "    m[i] = np.sum(w[doc_list[i]])\n",
    "    d_id.append(np.repeat(i, m[i]))\n",
    "d_id = np.hstack((d_id))\n",
    "sentence_id = np.repeat(np.arange(M), w)\n",
    "pt_id = np.hstack(([np.arange(w[i]) for i in range(M)]))\n",
    "\n",
    "# 文書のインデックスを定義\n",
    "d_list = []\n",
    "sentence_list = []\n",
    "for i in range(d):\n",
    "    d_list.append(np.where(d_id==i)[0].astype(\"int\"))\n",
    "for i in range(M):\n",
    "    sentence_list.append(np.where(sentence_id==i)[0].astype(\"int\"))\n",
    "    \n",
    "# 語順のインデックスを定義\n",
    "max_pt = np.max(pt_id) + 1\n",
    "pt_list = []\n",
    "pt_n = np.repeat(0, max_pt)\n",
    "for j in range(max_pt):\n",
    "    pt_list.append(np.array(np.where(pt_id==j)[0], dtype=\"int\"))\n",
    "    pt_n[j] = pt_list[j].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータと応答変数を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前分布の定義\n",
    "# HMMの事前分布を定義\n",
    "alpha1 = np.repeat(1.0, k1)\n",
    "alpha2 = np.append(np.repeat(0.5, k1), 5.0)\n",
    "\n",
    "# トピック分布の事前分布\n",
    "beta1 = np.append(np.repeat(0.1, k21), np.repeat(0.075, k22))\n",
    "beta2 = np.append(np.repeat(0.05, k21), np.repeat(0.1, k22))\n",
    "\n",
    "# 単語分布の事前分布\n",
    "max_word = 30\n",
    "gamma11 = np.full((k1, v11), 0.01)\n",
    "gamma12 = np.full((k1, v12), 0.005)\n",
    "gamma11[index_k11, ] = 0.05\n",
    "gamma12[index_k12, ] = 0.05\n",
    "gamma1 = np.hstack((gamma11, gamma12))\n",
    "gamma21 = np.full((k2, v11), 0.0025)\n",
    "gamma22 = np.full((k2, v12), 0.001)\n",
    "gamma21[index_k21, ] = 0.025\n",
    "gamma22[index_k22, ] = 0.025\n",
    "gamma2 = np.hstack((gamma21, gamma22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータを生成\n",
    "# HMMの推移確率を生成\n",
    "pi1 = np.append(np.random.dirichlet(alpha1, 1), 0.0).reshape(-1)\n",
    "while True:\n",
    "    pi2 = np.random.dirichlet(alpha2, k1+1)\n",
    "    if (np.mean(pi2[:, k1]) > 0.45) & (np.mean(pi2[:, k1]) < 0.6):\n",
    "        break\n",
    "pit1 = pi1.copy(); pit2 = pi2.copy()\n",
    "\n",
    "# ディリクリ分布からトピック分布を生成\n",
    "kappa = np.random.normal(0, 0.75, v1)\n",
    "theta = np.vstack((np.random.dirichlet(beta1, v11), \n",
    "                   np.random.dirichlet(beta2, v12)))\n",
    "kappat = kappa.copy(); thetat = theta.copy()\n",
    "\n",
    "# 単語分布の事前分布\n",
    "psi = np.array([np.random.dirichlet(gamma1[j, ], 1).reshape(-1) for j in range(k1)])\n",
    "phi = np.array([np.random.dirichlet(gamma2[j, ], 1).reshape(-1) for j in range(k2)])\n",
    "psit = psi.copy(); phit = phi.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 応答変数を生成\n",
    "# 生成したデータの格納用配列\n",
    "S = np.zeros((N, k1+1), dtype=\"int\")\n",
    "s = np.repeat(0, N)\n",
    "Z = np.zeros((N, k2), dtype=\"int\")\n",
    "z = np.repeat(-1, N)\n",
    "word_id = np.repeat(0, N).astype(\"int16\")\n",
    "word_long = np.full((N, max_pt), -1, dtype=\"int16\")\n",
    "attention_id = np.repeat(-1, N).astype(\"int16\")\n",
    "\n",
    "# トピックと単語を生成\n",
    "for j in range(max_pt):\n",
    "\n",
    "    # 語順に応じた生成を実行\n",
    "    if j==0:\n",
    "        \n",
    "        # 語順が先頭の単語を生成\n",
    "        # 多項分布からsyntaxを生成\n",
    "        index = pt_list[j]\n",
    "        S[index, ] = np.random.multinomial(1, pi1, pt_n[j])\n",
    "        s[index] = np.dot(S[index, ], np.arange(k1+1))\n",
    "\n",
    "        # 単語を生成\n",
    "        word_id[index] = rmnom(psi[s[index], ], pt_n[j], v1, 0)\n",
    "        word_long[index, j] = word_id[index, ]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # 語順が2単語目以降の単語を生成\n",
    "        # 多項分布からsyntaxを生成\n",
    "        index = pt_list[j]\n",
    "        res = rmnom(pi2[s[index-1], ], pt_n[j], k1+1, 1)\n",
    "        S[index, ] = res[1]\n",
    "        s[index] = res[0]\n",
    "\n",
    "        # 単語履歴を更新\n",
    "        for q in range(j):\n",
    "            word_long[index, q] = word_long[index-1, q]\n",
    "        if j < max_word:\n",
    "            index_col = np.arange(j)\n",
    "        else:\n",
    "            index_col = np.arange(j-max_word, j)\n",
    "\n",
    "        # attentionの単語を選択\n",
    "        index_hmm = index[np.array(np.where(res[1][:, k1]==0)[0], dtype=\"int\")]\n",
    "        index_attention = index[np.array(np.where(res[1][:, k1]==1)[0], dtype=\"int\")]  \n",
    "        m1 = index_hmm.shape[0]\n",
    "        m2 = index_attention.shape[0]\n",
    "        \n",
    "        if m2 > 0:\n",
    "            candidate_word = word_long[index_attention-1, ][:, index_col]\n",
    "            logit = kappa[candidate_word, ]\n",
    "            prob = np.exp(logit) / np.sum(np.exp(logit), axis=1)[:, np.newaxis]\n",
    "            word = np.sum(candidate_word * rmnom(prob, m2, prob.shape[1], 1)[1], axis=1)\n",
    "            attention_id[index_attention] = word\n",
    "\n",
    "            # attentionからトピックを生成\n",
    "            res = rmnom(theta[word, ], word.shape[0], k2, 1)\n",
    "            Z[index_attention, ] = res[1]\n",
    "            z[index_attention] = res[0]\n",
    "\n",
    "        # 単語を生成\n",
    "        word_id[index_hmm] = rmnom(psi[s[index_hmm], ], m1, v1, 0)\n",
    "        word_id[index_attention] = rmnom(phi[z[index_attention], ], m2, v1, 0)\n",
    "        word_long[index, j] = word_id[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一部データに教師をつける\n",
    "supervised_prob = 0.25\n",
    "index_topic = np.where(s==k1)[0].astype(\"int\")\n",
    "y = np.repeat(0, N)\n",
    "y[index_topic] = np.random.binomial(1, 0.25, index_topic.shape[0])\n",
    "index_y0 = np.where(y==0)[0].astype(\"int\")\n",
    "index_y1 = np.where(y==1)[0].astype(\"int\")\n",
    "wd = word_id[index_y1]\n",
    "\n",
    "# attentionの単語集合をセット\n",
    "attention_id_ = attention_id.copy()\n",
    "attention_set = np.full((N, max_word), -1, dtype=\"int16\")\n",
    "for j in range(1, max_pt):\n",
    "    index = pt_list[j]\n",
    "    if j < max_word:\n",
    "        index_col = np.arange(max_word)\n",
    "    else:\n",
    "        index_col = np.arange(j-max_word, j)\n",
    "    attention_set[index, ] = word_long[index-1, ][:, index_col]\n",
    "word_set = attention_set.copy()\n",
    "word_set[y==1] = -1\n",
    "\n",
    "# 単語集合のインデックスを定義\n",
    "set_list = [j for j in range(max_word+1)]\n",
    "set_id = [j for j in range(max_word+1)] \n",
    "set_list[0] = np.array([])\n",
    "set_id[0] = np.array([]) \n",
    "for j in range(max_word):\n",
    "    set_list[j+1] = np.array(np.where(word_set[:, j]!=-1)[0], dtype=\"int\")\n",
    "    set_id[j+1] = word_set[set_list[j+1], j]\n",
    "set_index = np.where((pt_id!=0) & (y!=1))[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentionの潜在変数行列を定義\n",
    "attention_index = np.where(attention_id!=-1)[0].astype(\"int\")\n",
    "D = np.full((N, max_word), 0)\n",
    "for i in range(attention_index.shape[0]):\n",
    "    index = attention_index[i]\n",
    "    target_index = np.where(attention_set[index, ]==attention_id_[index])[0].astype(\"int\")\n",
    "    D[index, target_index[0]] = 1\n",
    "d_supervised = D[index_y1, ]\n",
    "allocation_id = np.sum(D * attention_set, axis=1)[attention_index, ]\n",
    "\n",
    "# 依存単語の頻度を数える\n",
    "kappa = np.repeat(0, v1)\n",
    "freq = np.unique(allocation_id, return_counts=True)\n",
    "kappa[freq[0]] = freq[1]\n",
    "kappat = kappa.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Self Attention LDAを推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アルゴリズムの設定\n",
    "R = 1000\n",
    "keep = 2\n",
    "burnin = 500\n",
    "skeep = int(burnin/keep)\n",
    "iters = 0\n",
    "disp = 10\n",
    "latent_n = k1 + max_word\n",
    "latent_vec = np.repeat(1.0, latent_n)\n",
    "max_vec = np.repeat(1, max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスの定義\n",
    "# 単語のインデックスを定義\n",
    "word_list1 = [i for i in range(v11)]\n",
    "word_list2 = [i for i in range(v12)]\n",
    "word_vec1 = [i for i in range(v11)]\n",
    "word_vec2 = [i for i in range(v12)]\n",
    "for i in range(v11):\n",
    "    word_list1[i] = np.where(word_id==i)[0].astype(\"int\")\n",
    "    word_vec1[i] = np.repeat(1, word_list1[i].shape[0])\n",
    "for i in range(v12):\n",
    "    word_list2[i] = np.where(word_id==v11+i)[0].astype(\"int\")\n",
    "    word_vec2[i] = np.repeat(1, word_list2[i].shape[0])\n",
    "    \n",
    "# 先頭と末尾のインデックスを定義\n",
    "max_pt = np.max(pt_id) + 1\n",
    "index_t11 = np.where(pt_id==0)[0].astype(\"int\")\n",
    "index_t12 = np.repeat(0, d)\n",
    "for i in range(d):\n",
    "    index_t12[i] = np.max(d_list[i])\n",
    "    \n",
    "# 中間のインデックスを定義\n",
    "index_list_t21 = [j for j in range(max_pt-1)]\n",
    "index_list_t22 = [j for j in range(max_pt-1)]\n",
    "for j in range(1, max_pt):\n",
    "    index_list_t21[j-1] = np.where(pt_id==j)[0].astype(\"int\") - 1\n",
    "    index_list_t22[j-1] = np.where(pt_id==j)[0].astype(\"int\")\n",
    "index_t21 = np.sort(np.array(list(itertools.chain(*[index_list_t21[j] for j in range(max_pt-1)]))))\n",
    "index_t22 = np.sort(np.array(list(itertools.chain(*[index_list_t22[j] for j in range(max_pt-1)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの事前分布を定義\n",
    "# HMMの事前分布を定義\n",
    "alpha1 = np.repeat(1.0, k1)\n",
    "alpha2 = np.repeat(1.0, k1+1)\n",
    "\n",
    "# トピック分布の事前分布\n",
    "beta1 = 0.1\n",
    "beta2 = 0.1\n",
    "\n",
    "# 単語分布の事前分布\n",
    "gamma1 = 0.05\n",
    "gamma2 = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの真値\n",
    "# 推移確率とトピック分布の真値\n",
    "pi1 = pit1.copy()\n",
    "pi2 = pit2.copy()\n",
    "kappa = kappat.copy()\n",
    "theta = thetat.copy()\n",
    "\n",
    "# 単語分布の真値\n",
    "psi1 = psit[:, index_v11] / np.sum(psit[:, index_v11], axis=1)[:, np.newaxis]\n",
    "psi2 = psit[:, index_v12] / np.sum(psit[:, index_v12], axis=1)[:, np.newaxis]\n",
    "psi = np.hstack((psi1, psi2))\n",
    "phi1 = phit[:, index_v11] / np.sum(phit[:, index_v11], axis=1)[:, np.newaxis]\n",
    "phi2 = phit[:, index_v12] / np.sum(phit[:, index_v12], axis=1)[:, np.newaxis]\n",
    "phi = np.hstack((phi1, phi2))\n",
    "\n",
    "# 潜在変数の真値\n",
    "Si = S.copy()\n",
    "Di = D.copy()\n",
    "Zi = Z.copy()\n",
    "s = np.dot(Si, np.arange(k1+1))\n",
    "z = np.dot(Zi, np.arange(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの初期値\n",
    "# 推移確率とトピック分布の初期値\n",
    "pi1 = np.append(np.random.dirichlet(np.repeat(1.0, k1), 1), 0.0).reshape(-1)\n",
    "pi2 = np.random.dirichlet(np.repeat(1.0, k1+1), k1+1)\n",
    "theta = np.vstack((np.random.dirichlet(np.repeat(beta1, k2), v11), \n",
    "                   np.random.dirichlet(np.repeat(beta2, k2), v12)))\n",
    "\n",
    "# 単語ごとのattentionの事前分布\n",
    "n = int(np.mean(pi2[:, k1])*N)\n",
    "prob = np.random.dirichlet(np.repeat(10.0, v1), 1).reshape(-1)\n",
    "kappa = np.random.multinomial(n, prob, 1).reshape(-1)\n",
    "\n",
    "# 単語分布の初期値\n",
    "psi1 = np.random.dirichlet(np.repeat(1.0, v11), k1)\n",
    "psi2 = np.random.dirichlet(np.repeat(1.0, v12), k1)\n",
    "psi = np.hstack((psi1, psi2))\n",
    "phi1 = np.random.dirichlet(np.repeat(1.0, v11), k2)\n",
    "phi2 = np.random.dirichlet(np.repeat(1.0, v12), k2)\n",
    "phi = np.hstack((phi1, phi2))\n",
    "\n",
    "# 潜在変数の初期値\n",
    "Si = np.zeros((N, k1+1), dtype=\"int\")\n",
    "Si[index_y1, k1] = 1\n",
    "Si[index_y0, ] = np.random.multinomial(1, np.random.dirichlet(np.repeat(1.0, k1+1), 1).reshape(-1), index_y0.shape[0])\n",
    "Zi = np.random.multinomial(1, np.random.dirichlet(np.repeat(1.0, k2), 1).reshape(-1), N)\n",
    "s = np.dot(Si, np.arange(k1+1))\n",
    "z = np.dot(Zi, np.arange(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの格納用配列\n",
    "# バーンインのインデッククを定義\n",
    "RS = np.arange(skeep, int(R/keep))\n",
    "rs = RS.shape[0]\n",
    "\n",
    "# 推移確率とトピック分布の格納用配列\n",
    "PI1 = np.zeros((rs, k1+1))\n",
    "PI2 = np.zeros((k1+1, k1+1, rs))\n",
    "THETA = np.zeros((v1, k2, rs))\n",
    "\n",
    "# モデルパラメータの格納用配列\n",
    "PSI = np.zeros((k1, v1, rs))\n",
    "PHI = np.zeros((k2, v1, rs))\n",
    "\n",
    "# 潜在変数の格納用配列\n",
    "SEG1 = np.zeros((N, k1+1))\n",
    "SEG2 = np.zeros((N, max_word))\n",
    "SEG3 = np.zeros((N, k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対数尤度の基準値\n",
    "# ユニグラムモデルの基準値\n",
    "freq = np.unique(word_id, return_counts=True)\n",
    "par1 = np.repeat(0.0, v11)\n",
    "par2 = np.repeat(0.0, v12)\n",
    "par1[freq[0][freq[0] < v11]] = freq[1][freq[0] < v11]\n",
    "par2[freq[0][freq[0] >= v11] - v11] = freq[1][freq[0] >= v11]\n",
    "LLst1 = np.sum(np.log((par1 / np.sum(par1))[word_id[word_id < v11]]))\n",
    "LLst2 = np.sum(np.log((par2 / np.sum(par2))[word_id[word_id >= v11] - v11]))\n",
    "LLst = LLst1 + LLst2\n",
    "print(np.round([LLst1, LLst2, LLst], 1))\n",
    "\n",
    "# 真値での対数尤度の基準値\n",
    "index_syntax = np.where(S[:, k1]==0)[0].astype(\"int\")\n",
    "index_topic = np.where(S[:, k1]==1)[0].astype(\"int\")\n",
    "LLbest1 = np.sum(np.log(np.sum(S[index_syntax, :k1] * (psit.T)[word_id[index_syntax], ], axis=1)))\n",
    "LLbest2 = np.sum(np.log(np.sum(Z[index_topic, ] * (phit.T)[word_id[index_topic], ], axis=1)))\n",
    "LLbest = LLbest1 + LLbest2\n",
    "print(np.round([LLbest1, LLbest2, LLbest], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータを推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ギブスサンプリングでパラメータをサンプリング\n",
    "start_time = time.time()\n",
    "for rp in range(R):\n",
    "    \n",
    "    # 事後分布から潜在変数を生成\n",
    "    # syntaxとトピックの尤度を定義\n",
    "    phi_long = (phi.T)[word_id, ]\n",
    "    Lho1 = (psi.T)[word_id, ]   # syntaxごとの尤度\n",
    "    Lho2 = np.full((N, max_word), 0.0)   # attention wordごとの尤度\n",
    "    Prior_sum = np.full((N, max_word), 0)   # 事前分布の頻度\n",
    "    for j in range(1, max_word+1):\n",
    "        index = set_list[j]\n",
    "        Lho2[index, j-1] = np.dot(theta[set_id[j], ] * phi_long[index, ], k_vec2)\n",
    "        Prior_sum[index, j-1] = kappa[set_id[j]]\n",
    "    Prior_sum = Prior_sum[set_index, ]\n",
    "\n",
    "    # 推移確率とattentionの事前分布を定義\n",
    "    prior_pi1 = np.full((N, k1+1), 1/(k1+1)); prior_pi2 = prior_pi1.copy()\n",
    "    prior_pi1[index_t11, ] = np.full((M, k1+1), pi1)   # 文書の先頭と末尾の混合率\n",
    "    prior_pi1[index_t22, ] = pi2[s[index_t21], ]   # 1単語前の混合率\n",
    "    prior_pi2[index_t21, ]= (pi2.T)[s[index_t22], ]   # 1単語後の混合率\n",
    "    prior_kappa = Prior_sum / np.sum(Prior_sum, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 事前分布を結合\n",
    "    Prior1 = prior_pi1[:, :k1] * prior_pi2[:, :k1]\n",
    "    Prior2 = np.full((N, max_word), 0.0)\n",
    "    Prior2[set_index, ] = (prior_pi1[set_index, k1] * prior_pi2[set_index, k1])[:, np.newaxis] * prior_kappa\n",
    "\n",
    "    # 事後分布を定義\n",
    "    Posterior1 = Prior1 * Lho1\n",
    "    Posterior2 = Prior2 * Lho2\n",
    "    Posterior = np.hstack((Posterior1, Posterior2))[index_y0, ]\n",
    "\n",
    "    # 多項分布から潜在変数を生成\n",
    "    Prob = Posterior / np.dot(Posterior, latent_vec)[:, np.newaxis]\n",
    "    res = rmnom(Prob, index_y0.shape[0], latent_n, 1)\n",
    "    Si = np.full((N, k1+1), 0)\n",
    "    Si[index_y0, ] = np.hstack((res[1][:, :k1], np.dot(res[1][:, k1:], np.repeat(1, max_word))[:, np.newaxis]))\n",
    "    Si[index_y1, k1] = 1\n",
    "    s = np.dot(Si, np.arange(k1+1))\n",
    "    Di = np.full((N, max_word), 0)\n",
    "    Di[index_y0, ] = res[1][:, k1:]\n",
    "    Di[index_y1, ] = d_supervised\n",
    "\n",
    "\n",
    "    # 事前分布のパラメータをサンプリング\n",
    "    # ディリクレ分布から推移確率をサンプリング\n",
    "    rf1 = np.sum(Si[index_t11, ], axis=0)[:k1] + alpha1\n",
    "    rf2 = np.dot(Si[index_t21, ].T, Si[index_t22, ]) + alpha2\n",
    "    pi1 = np.append(np.random.dirichlet(rf1, 1).reshape(-1), 0.0)\n",
    "    pi2 = np.zeros((k1+1, k1+1))\n",
    "    for j in range(k1+1):\n",
    "        pi2[j, ] = np.random.dirichlet(rf2[j, ], 1).reshape(-1)\n",
    "\n",
    "    # attentionのトピック分布をサンプリング\n",
    "    index_syntax = np.where(s!=k1)[0].astype(\"int\")\n",
    "    index_topic = np.where(s==k1)[0].astype(\"int\")\n",
    "    attention_id = np.repeat(-1, N).astype(\"int16\")\n",
    "    attention_id[index_topic] = np.dot((Di[index_topic, ] * attention_set[index_topic, ]), max_vec)\n",
    "    freq = np.unique(attention_id[index_topic], return_counts=True)\n",
    "    kappa = np.repeat(0, v1)\n",
    "    kappa[freq[0]] = freq[1]\n",
    "\n",
    "\n",
    "    # attentionのトピックをサンプリング\n",
    "    # 事後分布を定義\n",
    "    n = index_topic.shape[0]\n",
    "    Posterior = theta[attention_id[index_topic], ] * phi_long[index_topic, ]   \n",
    "\n",
    "    # 多項分布からトピックをサンプリング\n",
    "    Prob = Posterior / np.dot(Posterior, k_vec2)[:, np.newaxis]\n",
    "    Zi = np.zeros((N, k2), dtype=\"int\")\n",
    "    Zi[index_topic, ] = rmnom(Prob, n, k2, 1)[1]\n",
    "\n",
    "    # トピック分布をサンプリング\n",
    "    attention_matrix = scipy.sparse.csr_matrix((np.repeat(1, n), (attention_id[index_topic], np.arange(n))), shape=(v1, n))\n",
    "    Zi_matrix = scipy.sparse.csr_matrix(Zi[index_topic, ])\n",
    "    wsum = np.dot(attention_matrix, Zi_matrix).toarray() + beta1\n",
    "    for i in range(v1):\n",
    "        theta[i, ] = np.random.dirichlet(wsum[i, ], 1).reshape(-1)\n",
    "\n",
    "\n",
    "    # 単語分布をサンプリング\n",
    "    # 事後分布のパラメータを定義\n",
    "    vf11 = np.zeros((k1, v11)); vf12 = np.zeros((k1, v12))\n",
    "    vf21 = np.zeros((k2, v11)); vf22 = np.zeros((k2, v12))\n",
    "    for j in range(v11):\n",
    "        index = word_list1[j]\n",
    "        vf11[:, j] = np.dot(Si[index, :k1].T, word_vec1[j]) + gamma1\n",
    "        vf21[:, j] = np.dot(Zi[index, ].T, word_vec1[j]) + gamma1\n",
    "    for j in range(v12):\n",
    "        index = word_list2[j]\n",
    "        vf12[:, j] = np.dot(Si[index, :k1].T, word_vec2[j]) + gamma2\n",
    "        vf22[:, j] = np.dot(Zi[index, ].T, word_vec2[j]) + gamma2\n",
    "\n",
    "    # ディリレク分布からパラメータをサンプリング\n",
    "    for j in range(k1):\n",
    "        psi1[j, ] = np.random.dirichlet(vf11[j, ], 1).reshape(-1) \n",
    "        psi2[j, ] = np.random.dirichlet(vf12[j, ], 1).reshape(-1) \n",
    "    psi = np.hstack((psi1, psi2))\n",
    "    for j in range(k2):\n",
    "        phi1[j, ] = np.random.dirichlet(vf21[j, ], 1).reshape(-1) \n",
    "        phi2[j, ] = np.random.dirichlet(vf22[j, ], 1).reshape(-1) \n",
    "    phi = np.hstack((phi1, phi2))\n",
    "\n",
    "\n",
    "    # サンプリング結果の保存と表示\n",
    "    # パラメータの格納\n",
    "    if (rp%keep==0) & (rp >= burnin):\n",
    "        mkeep = int(rp/keep) - skeep\n",
    "        \n",
    "        # モデルパラメータの格納\n",
    "        PI1[mkeep, ] = pi1\n",
    "        PI2[:, :, mkeep] = pi2\n",
    "        THETA[:, :, mkeep] = theta\n",
    "        PSI[:, :, mkeep] = psi\n",
    "        PHI[:, :, mkeep] = phi\n",
    "        \n",
    "        # 潜在変数の格納\n",
    "        SEG1 += Si\n",
    "        SEG2 += Di\n",
    "        SEG3 += Zi\n",
    "        \n",
    "    # 対数尤度の更新と結果の表示\n",
    "    if rp%disp==0:    \n",
    "        \n",
    "        # 経過時間を取得\n",
    "        intermediate_time = time.time()\n",
    "        elapsed_time = (intermediate_time - start_time) / 60\n",
    "        \n",
    "        # 対数尤度の更新\n",
    "        LL1 = np.sum(np.log(np.sum(Si[index_syntax, :k1] * (psi.T)[word_id[index_syntax], ], axis=1)))\n",
    "        LL2 = np.sum(np.log(np.sum(Zi[index_topic, ] * (phi.T)[word_id[index_topic], ], axis=1)))\n",
    "        LL = LL1 + LL2\n",
    "\n",
    "        #サンプリング結果を確認\n",
    "        print(rp)\n",
    "        print(\"経過時間: {}\".format(elapsed_time))\n",
    "        print(np.round(np.mean(Si, axis=0), 3))\n",
    "        print(np.round([LL, LLst, LLbest], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
